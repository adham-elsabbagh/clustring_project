#!/usr/bin/env python
# coding: utf-8

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction import text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from nltk.tokenize import RegexpTokenizer
from nltk.stem.snowball import SnowballStemmer


#convert txt file to csv file
# list_of_lists = []
# with open('data') as f:
#     for line in f:
#         inner_list = [line.strip() for line in line.split('\t')]
#         list_of_lists.append(inner_list)

# data = pd.DataFrame(list_of_lists)
# data['headline_text']=data[0]
# data=data.iloc[:,-1]
# data.to_csv('data2.csv')
# print(data.head())
# print(df.info())



data=pd.read_csv('data2.csv')
print(data.head())
print(data.columns)

data[data['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)
data = data.drop_duplicates('headline_text')

#stop words
punc = ['.', ',', '"', "'", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',"%"]
stop_word = text.ENGLISH_STOP_WORDS.union(punc)
desc = data['headline_text'].values

#stemming
stemmer = SnowballStemmer('english')
tokenizer = RegexpTokenizer(r'\b(?!nbsp\b)\w+[a-zA-Z\']+')
#toknizing
def tokenize(text):
    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]

#tf-idf vectorization
vectorizer = TfidfVectorizer(stop_words = stop_word, tokenizer = tokenize, max_features = 400)
x = vectorizer.fit_transform(desc).todense()
print(x)
words = vectorizer.get_feature_names()
print(len(words))
print(words)


#plotting elbow method to detect the k of clusters
wcss = []
max_=15
for i in range(1,max_):
    kmeans = KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=42,n_jobs=3)
    kmeans.fit(x)
    wcss.append(kmeans.inertia_)
plt.plot(range(1,max_),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.savefig('elbow.png')
plt.show()


#k-means
num_clusters=7
kmeans = KMeans(n_clusters = num_clusters, n_init = 10, n_jobs = 3,max_iter=300,random_state=42).fit(x) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)


# # We look at 7 the clusters generated by k-means.

# common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]
# for num, centroid in enumerate(common_words):
#     print('Cluster ',str(num) + ' : ' + ', '.join(words[word] for word in centroid))

print("Top 10 terms per cluster:")
order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]
for i in range(num_clusters):
    top_ten_words = [words[ind] for ind in order_centroids[i, :10]]
    print("Cluster {}: {}".format(i, ', '.join(top_ten_words)))


# y_kmeans = kmeans.predict(x)
# print(y_kmeans)
# data['clusters']=y_kmeans
# data.head()

#the lapels
labels = kmeans.labels_ 
data['label'] = labels
with open('cluster.txt','w')as f:
    f.write(data['label'].to_string(header = False, index = False))
# from sklearn.metrics import silhouette_score
# silhouette_score(x, y_kmeans)



